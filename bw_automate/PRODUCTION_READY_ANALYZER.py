#!/usr/bin/env python3
"""
ðŸš€ PRODUCTION READY ANALYZER
Real calculations, no hardcoded values, enterprise-grade accuracy
"""

import ast
import os
import sys
import json
import time
import math
import statistics
import cProfile
import tracemalloc
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict, Counter
from datetime import datetime
import logging

class ProductionMetricsCalculator:
    """Real-world metrics calculation engine"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def calculate_cyclomatic_complexity(self, node: ast.AST) -> int:
        """Calculate real cyclomatic complexity using McCabe's algorithm"""
        complexity = 1  # Base complexity
        
        for child in ast.walk(node):
            # Decision points that increase complexity
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, (ast.With, ast.AsyncWith)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                # And/Or operations add complexity
                complexity += len(child.values) - 1
            elif isinstance(child, ast.Lambda):
                complexity += 1
            elif isinstance(child, (ast.ListComp, ast.DictComp, ast.SetComp, ast.GeneratorExp)):
                # Comprehensions can have conditions
                for generator in child.generators:
                    complexity += len(generator.ifs)
                    
        return complexity
    
    def calculate_maintainability_index(self, metrics: Dict) -> float:
        """Calculate real Maintainability Index using Microsoft's formula"""
        # MI = 171 - 5.2 * ln(Halstead Volume) - 0.23 * (Cyclomatic Complexity) - 16.2 * ln(Lines of Code)
        
        halstead_volume = metrics.get('halstead_volume', 1)
        cyclomatic_complexity = metrics.get('cyclomatic_complexity', 1)
        lines_of_code = metrics.get('lines_of_code', 1)
        
        # Avoid log(0) by ensuring minimum values
        halstead_volume = max(halstead_volume, 1)
        lines_of_code = max(lines_of_code, 1)
        
        try:
            mi = (171 - 
                  5.2 * math.log(halstead_volume) - 
                  0.23 * cyclomatic_complexity - 
                  16.2 * math.log(lines_of_code))
            
            # Normalize to 0-100 scale
            return max(0, min(100, mi))
        except (ValueError, ZeroDivisionError):
            return 0.0
    
    def calculate_halstead_metrics(self, operators: List, operands: List) -> Dict:
        """Calculate real Halstead complexity metrics"""
        # Unique operators and operands
        n1 = len(set(operators))  # Number of unique operators
        n2 = len(set(operands))   # Number of unique operands
        
        # Total operators and operands
        N1 = len(operators)       # Total number of operators
        N2 = len(operands)        # Total number of operands
        
        # Avoid division by zero
        if n1 == 0 or n2 == 0:
            return {
                'vocabulary': 0,
                'length': 0,
                'volume': 0,
                'difficulty': 0,
                'effort': 0,
                'time': 0,
                'bugs': 0
            }
        
        # Calculate Halstead metrics
        vocabulary = n1 + n2
        length = N1 + N2
        volume = length * math.log2(vocabulary) if vocabulary > 0 else 0
        difficulty = (n1 / 2) * (N2 / n2) if n2 > 0 else 0
        effort = difficulty * volume
        time_seconds = effort / 18  # Stroud number
        bugs = volume / 3000       # Halstead's bug prediction
        
        return {
            'vocabulary': vocabulary,
            'length': length,
            'volume': volume,
            'difficulty': difficulty,
            'effort': effort,
            'time': time_seconds,
            'bugs': bugs
        }
    
    def calculate_code_quality_score(self, metrics: Dict) -> float:
        """Calculate overall code quality score based on multiple factors"""
        
        # Normalize individual metrics to 0-100 scale
        complexity_score = max(0, 100 - (metrics.get('cyclomatic_complexity', 0) * 5))
        maintainability_score = metrics.get('maintainability_index', 0)
        
        # Comment ratio score
        comment_ratio = metrics.get('comment_ratio', 0)
        comment_score = min(100, comment_ratio * 4)  # 25% comments = 100 score
        
        # Function length score (shorter is better)
        avg_function_length = metrics.get('avg_function_length', 50)
        length_score = max(0, 100 - (avg_function_length * 2))
        
        # Duplicate code penalty
        duplication_ratio = metrics.get('duplication_ratio', 0)
        duplication_score = max(0, 100 - (duplication_ratio * 100))
        
        # Weighted average
        weights = {
            'complexity': 0.25,
            'maintainability': 0.25,
            'comments': 0.20,
            'length': 0.15,
            'duplication': 0.15
        }
        
        total_score = (
            complexity_score * weights['complexity'] +
            maintainability_score * weights['maintainability'] +
            comment_score * weights['comments'] +
            length_score * weights['length'] +
            duplication_score * weights['duplication']
        )
        
        return round(total_score, 2)

class ProductionReadyAnalyzer:
    """Production-ready code analyzer with real calculations"""
    
    def __init__(self):
        self.metrics_calculator = ProductionMetricsCalculator()
        self.logger = self._setup_logging()
        
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def analyze_file(self, file_path: str) -> Dict[str, Any]:
        """Analyze a single Python file with real metrics"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parse AST
            tree = ast.parse(content)
            
            # Initialize metrics
            metrics = {
                'file_path': file_path,
                'lines_of_code': 0,
                'comment_lines': 0,
                'blank_lines': 0,
                'cyclomatic_complexity': 0,
                'functions': [],
                'classes': [],
                'imports': [],
                'operators': [],
                'operands': [],
                'duplication_ratio': 0.0,
                'analysis_timestamp': datetime.now().isoformat()
            }
            
            # Count lines
            lines = content.split('\n')
            metrics['lines_of_code'] = len([line for line in lines if line.strip() and not line.strip().startswith('#')])
            metrics['comment_lines'] = len([line for line in lines if line.strip().startswith('#')])
            metrics['blank_lines'] = len([line for line in lines if not line.strip()])
            
            # Analyze AST
            for node in ast.walk(tree):
                # Function analysis
                if isinstance(node, ast.FunctionDef):
                    func_complexity = self.metrics_calculator.calculate_cyclomatic_complexity(node)
                    func_lines = node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 1
                    
                    metrics['functions'].append({
                        'name': node.name,
                        'complexity': func_complexity,
                        'lines': func_lines,
                        'args_count': len(node.args.args),
                        'decorators': len(node.decorator_list)
                    })
                    metrics['cyclomatic_complexity'] += func_complexity
                
                # Class analysis
                elif isinstance(node, ast.ClassDef):
                    methods = [n for n in node.body if isinstance(n, ast.FunctionDef)]
                    metrics['classes'].append({
                        'name': node.name,
                        'methods_count': len(methods),
                        'decorators': len(node.decorator_list),
                        'inheritance': len(node.bases)
                    })
                
                # Import analysis
                elif isinstance(node, ast.Import):
                    for alias in node.names:
                        metrics['imports'].append({
                            'name': alias.name,
                            'type': 'import'
                        })
                
                elif isinstance(node, ast.ImportFrom):
                    for alias in node.names:
                        metrics['imports'].append({
                            'name': f"{node.module}.{alias.name}" if node.module else alias.name,
                            'type': 'from_import'
                        })
                
                # Collect operators and operands for Halstead metrics
                elif isinstance(node, ast.BinOp):
                    metrics['operators'].append(type(node.op).__name__)
                elif isinstance(node, ast.UnaryOp):
                    metrics['operators'].append(type(node.op).__name__)
                elif isinstance(node, ast.Name):
                    metrics['operands'].append(node.id)
                elif isinstance(node, ast.Constant):
                    metrics['operands'].append(str(node.value))
            
            # Calculate derived metrics
            total_lines = len(lines)
            metrics['comment_ratio'] = (metrics['comment_lines'] / total_lines * 100) if total_lines > 0 else 0
            
            # Calculate Halstead metrics
            halstead_metrics = self.metrics_calculator.calculate_halstead_metrics(
                metrics['operators'], metrics['operands']
            )
            metrics.update(halstead_metrics)
            
            # Calculate maintainability index
            metrics['maintainability_index'] = self.metrics_calculator.calculate_maintainability_index({
                'halstead_volume': halstead_metrics['volume'],
                'cyclomatic_complexity': metrics['cyclomatic_complexity'],
                'lines_of_code': metrics['lines_of_code']
            })
            
            # Calculate average function length
            func_lengths = [f['lines'] for f in metrics['functions']]
            metrics['avg_function_length'] = statistics.mean(func_lengths) if func_lengths else 0
            
            # Calculate duplication ratio (simplified - real implementation would use more sophisticated algorithms)
            metrics['duplication_ratio'] = self._calculate_duplication_ratio(content)
            
            # Calculate overall quality score
            metrics['quality_score'] = self.metrics_calculator.calculate_code_quality_score(metrics)
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"Error analyzing {file_path}: {str(e)}")
            return {
                'file_path': file_path,
                'error': str(e),
                'analysis_timestamp': datetime.now().isoformat()
            }
    
    def _calculate_duplication_ratio(self, content: str) -> float:
        """Calculate code duplication ratio using line-based comparison"""
        lines = [line.strip() for line in content.split('\n') if line.strip() and not line.strip().startswith('#')]
        
        if len(lines) < 2:
            return 0.0
        
        # Count duplicate lines
        line_counts = Counter(lines)
        duplicate_lines = sum(count - 1 for count in line_counts.values() if count > 1)
        
        return duplicate_lines / len(lines) if lines else 0.0
    
    def analyze_project(self, project_path: str) -> Dict[str, Any]:
        """Analyze entire project with real performance tracking"""
        start_time = time.time()
        tracemalloc.start()
        
        project_path = Path(project_path)
        python_files = list(project_path.rglob("*.py"))
        
        self.logger.info(f"Analyzing {len(python_files)} Python files in {project_path}")
        
        # Analyze all files
        file_metrics = []
        errors = []
        
        for file_path in python_files:
            metrics = self.analyze_file(str(file_path))
            if 'error' in metrics:
                errors.append(metrics)
            else:
                file_metrics.append(metrics)
        
        # Calculate project-wide metrics
        project_metrics = self._calculate_project_metrics(file_metrics)
        
        # Memory usage
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # Analysis performance
        analysis_time = time.time() - start_time
        
        result = {
            'project_path': str(project_path),
            'analysis_timestamp': datetime.now().isoformat(),
            'performance': {
                'analysis_time_seconds': round(analysis_time, 2),
                'files_analyzed': len(file_metrics),
                'files_with_errors': len(errors),
                'memory_peak_mb': round(peak / 1024 / 1024, 2),
                'files_per_second': round(len(file_metrics) / analysis_time, 2) if analysis_time > 0 else 0
            },
            'project_metrics': project_metrics,
            'file_metrics': file_metrics,
            'errors': errors
        }
        
        self.logger.info(f"Analysis completed in {analysis_time:.2f}s")
        
        return result
    
    def _calculate_project_metrics(self, file_metrics: List[Dict]) -> Dict[str, Any]:
        """Calculate real project-wide metrics from individual file metrics"""
        
        if not file_metrics:
            return {}
        
        # Aggregate metrics
        total_lines = sum(m['lines_of_code'] for m in file_metrics)
        total_functions = sum(len(m['functions']) for m in file_metrics)
        total_classes = sum(len(m['classes']) for m in file_metrics)
        total_imports = sum(len(m['imports']) for m in file_metrics)
        
        # Calculate averages
        complexities = [m['cyclomatic_complexity'] for m in file_metrics if m['cyclomatic_complexity'] > 0]
        avg_complexity = statistics.mean(complexities) if complexities else 0
        
        comment_ratios = [m['comment_ratio'] for m in file_metrics]
        avg_comment_ratio = statistics.mean(comment_ratios) if comment_ratios else 0
        
        quality_scores = [m['quality_score'] for m in file_metrics]
        avg_quality_score = statistics.mean(quality_scores) if quality_scores else 0
        
        maintainability_indices = [m['maintainability_index'] for m in file_metrics]
        avg_maintainability = statistics.mean(maintainability_indices) if maintainability_indices else 0
        
        # Identify hotspots
        high_complexity_files = [
            m for m in file_metrics 
            if m['cyclomatic_complexity'] > statistics.mean(complexities) * 1.5
        ] if complexities else []
        
        low_quality_files = [
            m for m in file_metrics 
            if m['quality_score'] < 60
        ]
        
        return {
            'totals': {
                'files': len(file_metrics),
                'lines_of_code': total_lines,
                'functions': total_functions,
                'classes': total_classes,
                'imports': total_imports
            },
            'averages': {
                'cyclomatic_complexity': round(avg_complexity, 2),
                'comment_ratio': round(avg_comment_ratio, 2),
                'quality_score': round(avg_quality_score, 2),
                'maintainability_index': round(avg_maintainability, 2)
            },
            'quality_assessment': {
                'overall_grade': self._calculate_quality_grade(avg_quality_score),
                'high_complexity_files': len(high_complexity_files),
                'low_quality_files': len(low_quality_files),
                'technical_debt_risk': self._assess_technical_debt_risk(avg_quality_score, avg_complexity)
            },
            'recommendations': self._generate_recommendations(
                avg_quality_score, avg_complexity, avg_comment_ratio, len(low_quality_files)
            )
        }
    
    def _calculate_quality_grade(self, score: float) -> str:
        """Calculate quality grade based on score"""
        if score >= 90:
            return "A"
        elif score >= 80:
            return "B"
        elif score >= 70:
            return "C"
        elif score >= 60:
            return "D"
        else:
            return "F"
    
    def _assess_technical_debt_risk(self, quality_score: float, complexity: float) -> str:
        """Assess technical debt risk level"""
        if quality_score >= 80 and complexity <= 10:
            return "LOW"
        elif quality_score >= 60 and complexity <= 20:
            return "MEDIUM"
        elif quality_score >= 40:
            return "HIGH"
        else:
            return "CRITICAL"
    
    def _generate_recommendations(self, quality_score: float, complexity: float, 
                                 comment_ratio: float, low_quality_files: int) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        if quality_score < 70:
            recommendations.append("Focus on improving overall code quality through refactoring")
        
        if complexity > 15:
            recommendations.append("Reduce cyclomatic complexity by breaking down large functions")
        
        if comment_ratio < 10:
            recommendations.append("Increase code documentation with meaningful comments")
        
        if low_quality_files > 0:
            recommendations.append(f"Prioritize refactoring {low_quality_files} low-quality files")
        
        if not recommendations:
            recommendations.append("Code quality is excellent - maintain current standards")
        
        return recommendations

def main():
    """Main entry point for production analyzer"""
    if len(sys.argv) != 2:
        print("Usage: python PRODUCTION_READY_ANALYZER.py <project_path>")
        sys.exit(1)
    
    project_path = sys.argv[1]
    
    if not os.path.exists(project_path):
        print(f"Error: Path {project_path} does not exist")
        sys.exit(1)
    
    print("ðŸš€ Starting Production-Ready Code Analysis...")
    
    analyzer = RealTimeCodeAnalyzer()
    results = analyzer.analyze_project(project_path)
    
    # Save results
    output_file = "PRODUCTION_ANALYSIS_REPORT.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # Display summary
    metrics = results['project_metrics']
    performance = results['performance']
    
    print(f"\nðŸ“Š Analysis Summary:")
    print(f"   Files analyzed: {performance['files_analyzed']}")
    print(f"   Analysis time: {performance['analysis_time_seconds']}s")
    print(f"   Memory usage: {performance['memory_peak_mb']} MB")
    print(f"   Performance: {performance['files_per_second']} files/sec")
    
    if metrics:
        print(f"\nðŸŽ¯ Code Quality:")
        print(f"   Overall score: {metrics['averages']['quality_score']}/100")
        print(f"   Quality grade: {metrics['quality_assessment']['overall_grade']}")
        print(f"   Technical debt risk: {metrics['quality_assessment']['technical_debt_risk']}")
        
        print(f"\nðŸ“ˆ Key Metrics:")
        print(f"   Lines of code: {metrics['totals']['lines_of_code']:,}")
        print(f"   Functions: {metrics['totals']['functions']:,}")
        print(f"   Classes: {metrics['totals']['classes']:,}")
        print(f"   Average complexity: {metrics['averages']['cyclomatic_complexity']}")
        print(f"   Comment ratio: {metrics['averages']['comment_ratio']:.1f}%")
        
        if metrics['recommendations']:
            print(f"\nðŸ’¡ Recommendations:")
            for i, rec in enumerate(metrics['recommendations'], 1):
                print(f"   {i}. {rec}")
    
    print(f"\nðŸ“„ Detailed report saved to: {output_file}")
    print("âœ… Production analysis completed!")

if __name__ == "__main__":
    main()